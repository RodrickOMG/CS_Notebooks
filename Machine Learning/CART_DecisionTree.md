*本文为本人在学习机器学习决策树部分的一些笔记和思考，以及python编程实现算法的具体步骤*

**决策树(decision tree)** 是一类常见的机器学习方法. 在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的**期望值**大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种**图解法**. 在机器学习中，决策树是一个**预测模型**，他代表的是对象属性与对象值之间的一种**映射关系**.

对于决策树的原理和概念以及信息增益的计算部分不再赘述，对此部分感兴趣或者希望了解的朋友可以翻阅周志华《机器学习》P73～P78

## 本文重点介绍CART决策树

CART决策树[Breiman et al., 1984] 使用“基尼指数”来选择划分属性。这是西瓜书上给出的定义. 通过大量文章的阅读将CART决策树关键点整理如下：

 1. CART决策树既能是分类树，也能是回归树
 2. 当CART是分类树时，采用GINI值作为节点分裂的依据；当CART是回归树时，采用样本的最小方差作为节点分裂的依据
 3. ==CART是一颗二叉树== (关于这一点其实我存在疑惑，准备去问问老师或者同学。因为在我编程实现的过程中忽略了这一点，因为西瓜树上并没有指明CART算法必须生成二叉树. 而其中西瓜数据集中的离散属性取值$N\geq3$，因此我在编程过程中生成的是多叉树. 所以是否考虑在属性取值较少的情况下，CART算法不用一定生成二叉树)

目标取值为一个有限集合的树模型称为**分类树**，而目标值是连续值(典型的真实数值)的树模型称为**回归树**。分裂的目的是为了能够让数据变纯，使决策树输出的结果更接近真实值。那么CART是如何评价节点的纯度呢？如果是分类树，CART采用GINI值衡量节点纯度；如果是回归树，采用样本方差衡量节点纯度. 节点越不纯，节点分类或者预测的效果就越差.

### 1、CART决策树作为分类树 
CART决策树作为分类树时，特征属性可以是连续类型也可以是离散类型，但观察属性(即标签属性或者分类属性)必须是离散类型。
划分的目的是为了能够让数据变纯，使决策树输出的结果更接近真实值。如果是分类树，CART决策树使用“基尼指数”来选择划分属性，数据集D的纯度可以用基尼值来度量：

$$Gini(D)=\sum_{k=1}^{\vert \mathcal Y\vert}\sum_{k'\neq k}p_kp_{k'}$$

$$=1-\sum_{k=1}^{\vert \mathcal Y\vert}p_k^2  $$

直观来说，$Gini(D)$反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(D)$越小，则数据集$D$的纯度越高.